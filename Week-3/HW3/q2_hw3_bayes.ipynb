{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Problem 2 - Naive Bayes",
   "id": "4bdf710cb61ad665"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T23:43:06.671370Z",
     "start_time": "2025-10-11T23:43:06.668083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "id": "20b6d56c50fda139",
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-11T23:43:06.690293Z",
     "start_time": "2025-10-11T23:43:06.678404Z"
    }
   },
   "source": [
    "data = np.loadtxt(\"spambase/spambase.data\", delimiter=\",\")\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1].astype(int)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Gaussian Naive Bayes Classifier:",
   "id": "565d639ad3c18f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T23:43:06.696025Z",
     "start_time": "2025-10-11T23:43:06.693156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.priors = None\n",
    "        self.means = None\n",
    "        self.variances = None\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.means = np.zeros((len(self.classes), n_features))\n",
    "        self.variances = np.zeros((len(self.classes), n_features))\n",
    "        self.priors = np.zeros(len(self.classes))\n",
    "\n",
    "        # each class calculation for prior, mean and variance\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y==c]\n",
    "\n",
    "            self.priors[idx] = len(X_c) / n_samples\n",
    "            self.means[idx, :] = np.mean(X_c, axis=0)\n",
    "            self.variances[idx, :] = np.var(X_c, axis=0) + 1e-9\n",
    "\n",
    "    def gaussian(self, x, mean, variance):\n",
    "        return (1.0 / np.sqrt(2*np.pi*variance)) * np.exp(-((x-mean)**2) / (2 * variance))\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\"Single instance\"\"\"\n",
    "        posteriors = []\n",
    "\n",
    "        # Calculate P(y=c|x) for each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Start with prior P(y=c)\n",
    "            posterior = self.priors[idx]\n",
    "\n",
    "            # Multiply by P(xi|y=c) for each feature\n",
    "            for i in range(len(x)):\n",
    "                likelihood = self.gaussian(\n",
    "                    x[i],\n",
    "                    self.means[idx, i],\n",
    "                    self.variances[idx, i]\n",
    "                )\n",
    "                posterior *= likelihood\n",
    "\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Multiple instances\"\"\"\n",
    "        return np.array([self.predict_single(x) for x in X])"
   ],
   "id": "db46df3c1191d2e7",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bernoulli Classifier:",
   "id": "55c24bbd03522036"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T23:43:06.701522Z",
     "start_time": "2025-10-11T23:43:06.698624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BernoulliNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_priors = None\n",
    "        self.means = None  # 2x57 matrix of feature means per class\n",
    "        self.probs = None  # 2x57 matrix Z[k,i] = P(feature i < mean | class k)\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train BNB by computing class-specific means and Bernoulli probabilities\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize matrices\n",
    "        self.means = np.zeros((len(self.classes), n_features))\n",
    "        self.probs = np.zeros((len(self.classes), n_features))\n",
    "        self.class_priors = np.zeros(len(self.classes))\n",
    "\n",
    "        # Compute statistics for each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "\n",
    "            # Class prior P(y=c)\n",
    "            self.class_priors[idx] = len(X_c) / n_samples\n",
    "\n",
    "            # Mean for each feature\n",
    "            self.means[idx, :] = np.mean(X_c, axis=0)\n",
    "\n",
    "            # Z[k,i] = probability of being below mean for feature i in class k\n",
    "            for i in range(n_features):\n",
    "                # Count how many samples are below the mean\n",
    "                below_mean = X_c[:, i] <= self.means[idx, i]\n",
    "                self.probs[idx, i] = np.mean(below_mean)\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predict single instance using Bernoulli distribution\n",
    "        \"\"\"\n",
    "        posteriors = []\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Start with prior P(y=c)\n",
    "            posterior = self.class_priors[idx]\n",
    "\n",
    "            # Multiply by P(xi|y=c) for each feature\n",
    "            for i in range(len(x)):\n",
    "                # Bernoulli probability based on whether xi > mean[k,i]\n",
    "                if x[i] > self.means[idx, i]:\n",
    "                    # P(xi > mean | y=c) = 1 - Z[k,i]\n",
    "                    prob = 1 - self.probs[idx, i]\n",
    "                else:\n",
    "                    # P(xi <= mean | y=c) = Z[k,i]\n",
    "                    prob = self.probs[idx, i]\n",
    "\n",
    "                # Avoid zero probability\n",
    "                prob = max(prob, 1e-10)\n",
    "                posterior *= prob\n",
    "\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict multiple instances\n",
    "        \"\"\"\n",
    "        return np.array([self.predict_single(x) for x in X])"
   ],
   "id": "20512028c8714010",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Non-Parametric Classifier: Histogram 4 bin",
   "id": "2ba94614af6c4a8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T23:43:06.707768Z",
     "start_time": "2025-10-11T23:43:06.704113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HistogramNaiveBayes:\n",
    "    def __init__(self, n_bins=4, alpha=0.01, use_log=True):\n",
    "        \"\"\"\n",
    "        Final implementation with log1p preprocessing\n",
    "        - Strictly 4 bins as required by TA\n",
    "        - Log1p transformation handles zeros and skewed distributions\n",
    "        \"\"\"\n",
    "        self.n_bins = n_bins\n",
    "        self.alpha = alpha\n",
    "        self.use_log = use_log\n",
    "        self.class_priors = None\n",
    "        self.bin_edges = {}\n",
    "        self.bin_probs = {}\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train 4-bin Histogram NB with log1p preprocessing\n",
    "        \"\"\"\n",
    "        # Apply log1p transformation if enabled\n",
    "        if self.use_log:\n",
    "            X_processed = np.log1p(X)\n",
    "        else:\n",
    "            X_processed = X\n",
    "\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples, n_features = X_processed.shape\n",
    "        self.class_priors = np.zeros(len(self.classes))\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X_processed[y == c]\n",
    "            self.class_priors[idx] = len(X_c) / n_samples\n",
    "\n",
    "            self.bin_edges[c] = {}\n",
    "            self.bin_probs[c] = {}\n",
    "\n",
    "            for feature_idx in range(n_features):\n",
    "                feature_values = X_c[:, feature_idx]\n",
    "\n",
    "                # Create 4-bin histogram with TA-specified edges:\n",
    "                # min, Q1, mean, Q3, max\n",
    "                min_val = np.min(feature_values)\n",
    "                q1 = np.percentile(feature_values, 25)\n",
    "                mean_val = np.mean(feature_values)\n",
    "                q3 = np.percentile(feature_values, 75)\n",
    "                max_val = np.max(feature_values)\n",
    "\n",
    "                edges = np.array([min_val, q1, mean_val, q3, max_val])\n",
    "                edges = np.sort(edges)\n",
    "                edges = np.unique(edges)\n",
    "\n",
    "                # Ensure we have 5 edges for 4 bins\n",
    "                if len(edges) < 5:\n",
    "                    edges = np.linspace(min_val, max_val + 1e-10, 5)\n",
    "\n",
    "                edges[-1] += 1e-10  # Ensure max is included\n",
    "\n",
    "                # Calculate histogram\n",
    "                hist, _ = np.histogram(feature_values, bins=edges)\n",
    "\n",
    "                # Apply Laplace smoothing\n",
    "                hist_smooth = hist + self.alpha\n",
    "\n",
    "                # Normalize to probabilities\n",
    "                hist_prob = hist_smooth / np.sum(hist_smooth)\n",
    "\n",
    "                self.bin_edges[c][feature_idx] = edges\n",
    "                self.bin_probs[c][feature_idx] = hist_prob\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predict single instance\n",
    "        \"\"\"\n",
    "        log_posteriors = []\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            log_posterior = np.log(self.class_priors[idx] + 1e-10)\n",
    "\n",
    "            for i in range(len(x)):\n",
    "                edges = self.bin_edges[c][i]\n",
    "                probs = self.bin_probs[c][i]\n",
    "\n",
    "                bin_idx = np.searchsorted(edges[:-1], x[i], side='right') - 1\n",
    "                bin_idx = np.clip(bin_idx, 0, len(probs) - 1)\n",
    "\n",
    "                log_posterior += np.log(probs[bin_idx] + 1e-10)\n",
    "\n",
    "            log_posteriors.append(log_posterior)\n",
    "\n",
    "        return self.classes[np.argmax(log_posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict with same preprocessing\n",
    "        \"\"\"\n",
    "        # Apply same preprocessing\n",
    "        if self.use_log:\n",
    "            X_processed = np.log1p(X)\n",
    "        else:\n",
    "            X_processed = X\n",
    "\n",
    "        return np.array([self.predict_single(x) for x in X_processed])"
   ],
   "id": "ae898492313359c9",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "K-FOLD Cross Val:",
   "id": "4435aa1d5cbc278b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T23:43:17.880841Z",
     "start_time": "2025-10-11T23:43:06.711678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model_kfold(model_class, X, y, n_folds=5):\n",
    "    \"\"\"\n",
    "    Evaluate model using k-fold cross validation\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "        # Split data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Normalize data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_norm = scaler.fit_transform(X_train)\n",
    "        X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "        # Train model\n",
    "        model = model_class()\n",
    "        model.fit(X_train_norm, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred_train = model.predict(X_train_norm)\n",
    "        y_pred_test = model.predict(X_test_norm)\n",
    "\n",
    "        train_acc = accuracy_score(y_train, y_pred_train)\n",
    "        test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        print(f\"Fold {fold}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "\n",
    "    avg_train = np.mean(train_accuracies)\n",
    "    avg_test = np.mean(test_accuracies)\n",
    "    std_train = np.std(train_accuracies)\n",
    "    std_test = np.std(test_accuracies)\n",
    "\n",
    "    return {\n",
    "        'train_accs': train_accuracies,\n",
    "        'test_accs': test_accuracies,\n",
    "        'avg_train': avg_train,\n",
    "        'avg_test': avg_test,\n",
    "        'std_train': std_train,\n",
    "        'std_test': std_test\n",
    "    }\n",
    "\n",
    "# Evaluate all three models\n",
    "print(\"=\"*50)\n",
    "print(\"Gaussian Naive Bayes Results:\")\n",
    "print(\"=\"*50)\n",
    "gnb_results = evaluate_model_kfold(GaussianNaiveBayes, X, y, n_folds=5)\n",
    "print(f\"\\nAverage Train Accuracy: {gnb_results['avg_train']:.4f} ± {gnb_results['std_train']:.4f}\")\n",
    "print(f\"Average Test Accuracy: {gnb_results['avg_test']:.4f} ± {gnb_results['std_test']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Bernoulli Naive Bayes Results:\")\n",
    "print(\"=\"*50)\n",
    "bnb_results = evaluate_model_kfold(BernoulliNaiveBayes, X, y, n_folds=5)\n",
    "print(f\"\\nAverage Train Accuracy: {bnb_results['avg_train']:.4f} ± {bnb_results['std_train']:.4f}\")\n",
    "print(f\"Average Test Accuracy: {bnb_results['avg_test']:.4f} ± {bnb_results['std_test']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Histogram Naive Bayes Results:\")\n",
    "print(\"=\"*50)\n",
    "hnb_results = evaluate_model_kfold(HistogramNaiveBayes, X, y, n_folds=5)\n",
    "print(f\"\\nAverage Train Accuracy: {hnb_results['avg_train']:.4f} ± {hnb_results['std_train']:.4f}\")\n",
    "print(f\"Average Test Accuracy: {hnb_results['avg_test']:.4f} ± {hnb_results['std_test']:.4f}\")"
   ],
   "id": "ee385507d47c0112",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Gaussian Naive Bayes Results:\n",
      "==================================================\n",
      "Fold 1: Train Acc = 0.8204, Test Acc = 0.8208\n",
      "Fold 2: Train Acc = 0.8047, Test Acc = 0.8033\n",
      "Fold 3: Train Acc = 0.8245, Test Acc = 0.7946\n",
      "Fold 4: Train Acc = 0.8250, Test Acc = 0.8228\n",
      "Fold 5: Train Acc = 0.8215, Test Acc = 0.8337\n",
      "\n",
      "Average Train Accuracy: 0.8192 ± 0.0075\n",
      "Average Test Accuracy: 0.8150 ± 0.0141\n",
      "\n",
      "==================================================\n",
      "Bernoulli Naive Bayes Results:\n",
      "==================================================\n",
      "Fold 1: Train Acc = 0.8938, Test Acc = 0.9001\n",
      "Fold 2: Train Acc = 0.8894, Test Acc = 0.8848\n",
      "Fold 3: Train Acc = 0.8970, Test Acc = 0.8880\n",
      "Fold 4: Train Acc = 0.8998, Test Acc = 0.8783\n",
      "Fold 5: Train Acc = 0.8946, Test Acc = 0.9065\n",
      "\n",
      "Average Train Accuracy: 0.8949 ± 0.0034\n",
      "Average Test Accuracy: 0.8915 ± 0.0103\n",
      "\n",
      "==================================================\n",
      "Histogram Naive Bayes Results:\n",
      "==================================================\n",
      "Fold 1: Train Acc = 0.8821, Test Acc = 0.8599\n",
      "Fold 2: Train Acc = 0.8824, Test Acc = 0.8967\n",
      "Fold 3: Train Acc = 0.8911, Test Acc = 0.8935\n",
      "Fold 4: Train Acc = 0.8911, Test Acc = 0.8913\n",
      "Fold 5: Train Acc = 0.8794, Test Acc = 0.8815\n",
      "\n",
      "Average Train Accuracy: 0.8852 ± 0.0049\n",
      "Average Test Accuracy: 0.8846 ± 0.0133\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T23:43:26.130153Z",
     "start_time": "2025-10-11T23:43:17.896644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the fixed Histogram NB\n",
    "print(\"=\"*50)\n",
    "print(\"Fixed Histogram Naive Bayes Results:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use the alternative implementation\n",
    "hnb_fixed_results = evaluate_model_kfold(HistogramNaiveBayes, X, y, n_folds=5)\n",
    "\n",
    "print(f\"\\nAverage Train Accuracy: {hnb_fixed_results['avg_train']:.4f} ± {hnb_fixed_results['std_train']:.4f}\")\n",
    "print(f\"Average Test Accuracy: {hnb_fixed_results['avg_test']:.4f} ± {hnb_fixed_results['std_test']:.4f}\")\n",
    "\n",
    "# Create updated summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY TABLE - All Models (5-Fold CV)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Train Acc':<15} {'Test Acc':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Gaussian NB':<20} {gnb_results['avg_train']:.4f} ± {gnb_results['std_train']:.4f}  \"\n",
    "      f\"{gnb_results['avg_test']:.4f} ± {gnb_results['std_test']:.4f}\")\n",
    "print(f\"{'Bernoulli NB':<20} {bnb_results['avg_train']:.4f} ± {bnb_results['std_train']:.4f}  \"\n",
    "      f\"{bnb_results['avg_test']:.4f} ± {bnb_results['std_test']:.4f}\")\n",
    "print(f\"{'Histogram NB (Fixed)':<20} {hnb_fixed_results['avg_train']:.4f} ± {hnb_fixed_results['std_train']:.4f}  \"\n",
    "      f\"{hnb_fixed_results['avg_test']:.4f} ± {hnb_fixed_results['std_test']:.4f}\")"
   ],
   "id": "4f804495e9ab6422",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Fixed Histogram Naive Bayes Results:\n",
      "==================================================\n",
      "Fold 1: Train Acc = 0.8821, Test Acc = 0.8599\n",
      "Fold 2: Train Acc = 0.8824, Test Acc = 0.8967\n",
      "Fold 3: Train Acc = 0.8911, Test Acc = 0.8935\n",
      "Fold 4: Train Acc = 0.8911, Test Acc = 0.8913\n",
      "Fold 5: Train Acc = 0.8794, Test Acc = 0.8815\n",
      "\n",
      "Average Train Accuracy: 0.8852 ± 0.0049\n",
      "Average Test Accuracy: 0.8846 ± 0.0133\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY TABLE - All Models (5-Fold CV)\n",
      "============================================================\n",
      "Model                Train Acc       Test Acc       \n",
      "------------------------------------------------------------\n",
      "Gaussian NB          0.8192 ± 0.0075  0.8150 ± 0.0141\n",
      "Bernoulli NB         0.8949 ± 0.0034  0.8915 ± 0.0103\n",
      "Histogram NB (Fixed) 0.8852 ± 0.0049  0.8846 ± 0.0133\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T23:43:26.148091Z",
     "start_time": "2025-10-11T23:43:26.146860Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e7da7e3a8b25f1cb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
