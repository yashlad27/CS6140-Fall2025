{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Problem 2 - Naive Bayes",
   "id": "4bdf710cb61ad665"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T19:24:52.694484Z",
     "start_time": "2025-10-11T19:24:52.685708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "id": "20b6d56c50fda139",
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-11T19:24:52.723385Z",
     "start_time": "2025-10-11T19:24:52.701518Z"
    }
   },
   "source": [
    "data = np.loadtxt(\"spambase/spambase.data\", delimiter=\",\")\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1].astype(int)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Gaussian Naive Bayes Classifier:",
   "id": "565d639ad3c18f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T19:24:52.730630Z",
     "start_time": "2025-10-11T19:24:52.727368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.priors = None\n",
    "        self.means = None\n",
    "        self.variances = None\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.means = np.zeros((len(self.classes), n_features))\n",
    "        self.variances = np.zeros((len(self.classes), n_features))\n",
    "        self.priors = np.zeros(len(self.classes))\n",
    "\n",
    "        # each class calculation for prior, mean and variance\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y==c]\n",
    "\n",
    "            self.priors[idx] = len(X_c) / n_samples\n",
    "            self.means[idx, :] = np.mean(X_c, axis=0)\n",
    "            self.variances[idx, :] = np.var(X_c, axis=0) + 1e-9\n",
    "\n",
    "    def gaussian(self, x, mean, variance):\n",
    "        return (1.0 / np.sqrt(2*np.pi*variance)) * np.exp(-((x-mean)**2) / (2 * variance))\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\"Single instance\"\"\"\n",
    "        posteriors = []\n",
    "\n",
    "        # Calculate P(y=c|x) for each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Start with prior P(y=c)\n",
    "            posterior = self.priors[idx]\n",
    "\n",
    "            # Multiply by P(xi|y=c) for each feature\n",
    "            for i in range(len(x)):\n",
    "                likelihood = self.gaussian(\n",
    "                    x[i],\n",
    "                    self.means[idx, i],\n",
    "                    self.variances[idx, i]\n",
    "                )\n",
    "                posterior *= likelihood\n",
    "\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Multiple instances\"\"\"\n",
    "        return np.array([self.predict_single(x) for x in X])"
   ],
   "id": "db46df3c1191d2e7",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bernoulli Classifier:",
   "id": "55c24bbd03522036"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T19:24:52.735197Z",
     "start_time": "2025-10-11T19:24:52.732354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BernoulliNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_priors = None\n",
    "        self.means = None  # 2x57 matrix of feature means per class\n",
    "        self.probs = None  # 2x57 matrix Z[k,i] = P(feature i < mean | class k)\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train BNB by computing class-specific means and Bernoulli probabilities\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize matrices\n",
    "        self.means = np.zeros((len(self.classes), n_features))\n",
    "        self.probs = np.zeros((len(self.classes), n_features))\n",
    "        self.class_priors = np.zeros(len(self.classes))\n",
    "\n",
    "        # Compute statistics for each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "\n",
    "            # Class prior P(y=c)\n",
    "            self.class_priors[idx] = len(X_c) / n_samples\n",
    "\n",
    "            # Mean for each feature\n",
    "            self.means[idx, :] = np.mean(X_c, axis=0)\n",
    "\n",
    "            # Z[k,i] = probability of being below mean for feature i in class k\n",
    "            for i in range(n_features):\n",
    "                # Count how many samples are below the mean\n",
    "                below_mean = X_c[:, i] <= self.means[idx, i]\n",
    "                self.probs[idx, i] = np.mean(below_mean)\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predict single instance using Bernoulli distribution\n",
    "        \"\"\"\n",
    "        posteriors = []\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Start with prior P(y=c)\n",
    "            posterior = self.class_priors[idx]\n",
    "\n",
    "            # Multiply by P(xi|y=c) for each feature\n",
    "            for i in range(len(x)):\n",
    "                # Bernoulli probability based on whether xi > mean[k,i]\n",
    "                if x[i] > self.means[idx, i]:\n",
    "                    # P(xi > mean | y=c) = 1 - Z[k,i]\n",
    "                    prob = 1 - self.probs[idx, i]\n",
    "                else:\n",
    "                    # P(xi <= mean | y=c) = Z[k,i]\n",
    "                    prob = self.probs[idx, i]\n",
    "\n",
    "                # Avoid zero probability\n",
    "                prob = max(prob, 1e-10)\n",
    "                posterior *= prob\n",
    "\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict multiple instances\n",
    "        \"\"\"\n",
    "        return np.array([self.predict_single(x) for x in X])"
   ],
   "id": "20512028c8714010",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Non-Parametric Classifier: Histogram 4 bin",
   "id": "2ba94614af6c4a8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T19:24:52.741193Z",
     "start_time": "2025-10-11T19:24:52.736991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HistogramNaiveBayes:\n",
    "    def __init__(self, n_bins=4, alpha=1.0):\n",
    "        self.n_bins = n_bins\n",
    "        self.alpha = alpha  # Laplace smoothing parameter\n",
    "        self.class_priors = None\n",
    "        self.bin_edges = {}\n",
    "        self.bin_probs = {}\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train HNB by computing class-specific 4-bin histograms\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.class_priors = np.zeros(len(self.classes))\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.class_priors[idx] = len(X_c) / n_samples\n",
    "\n",
    "            self.bin_edges[c] = {}\n",
    "            self.bin_probs[c] = {}\n",
    "\n",
    "            for feature_idx in range(n_features):\n",
    "                feature_values = X_c[:, feature_idx]\n",
    "\n",
    "                # CRITICAL FIX: Use mean instead of median as per TA specification\n",
    "                # Edges should be: [min, Q1, mean, Q3, max]\n",
    "                min_val = np.min(feature_values)\n",
    "                q1 = np.percentile(feature_values, 25)\n",
    "                mean_val = np.mean(feature_values)  # Changed from median to mean\n",
    "                q3 = np.percentile(feature_values, 75)\n",
    "                max_val = np.max(feature_values)\n",
    "\n",
    "                # Create edges array\n",
    "                edges = np.array([min_val, q1, mean_val, q3, max_val])\n",
    "\n",
    "                # Sort edges (important since mean might not be between Q1 and Q3)\n",
    "                edges = np.sort(edges)\n",
    "\n",
    "                # Remove duplicates while keeping at least n_bins+1 edges\n",
    "                unique_edges = []\n",
    "                for edge in edges:\n",
    "                    if len(unique_edges) == 0 or edge > unique_edges[-1] + 1e-10:\n",
    "                        unique_edges.append(edge)\n",
    "\n",
    "                # If we have fewer unique edges, create uniform bins\n",
    "                if len(unique_edges) < self.n_bins + 1:\n",
    "                    unique_edges = np.linspace(min_val,\n",
    "                                              max_val + 1e-10,\n",
    "                                              self.n_bins + 1).tolist()\n",
    "\n",
    "                edges = np.array(unique_edges)\n",
    "\n",
    "                # Ensure the last edge includes the maximum value\n",
    "                edges[-1] = max_val + 1e-10\n",
    "\n",
    "                # Calculate histogram\n",
    "                hist, _ = np.histogram(feature_values, bins=edges)\n",
    "\n",
    "                # Apply Laplace smoothing\n",
    "                hist_smooth = hist + self.alpha\n",
    "\n",
    "                # Normalize to get probabilities\n",
    "                hist_prob = hist_smooth / np.sum(hist_smooth)\n",
    "\n",
    "                self.bin_edges[c][feature_idx] = edges\n",
    "                self.bin_probs[c][feature_idx] = hist_prob\n",
    "\n",
    "    def get_bin_probability(self, value, class_label, feature_idx):\n",
    "        \"\"\"\n",
    "        Get bin probability for a value with proper handling\n",
    "        \"\"\"\n",
    "        edges = self.bin_edges[class_label][feature_idx]\n",
    "        probs = self.bin_probs[class_label][feature_idx]\n",
    "\n",
    "        # Find which bin the value falls into\n",
    "        # Use searchsorted for more reliable binning\n",
    "        bin_idx = np.searchsorted(edges[:-1], value, side='right') - 1\n",
    "\n",
    "        # Handle edge cases\n",
    "        if bin_idx < 0:\n",
    "            bin_idx = 0\n",
    "        elif bin_idx >= len(probs):\n",
    "            bin_idx = len(probs) - 1\n",
    "\n",
    "        return probs[bin_idx]\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predict single instance using histogram probabilities\n",
    "        \"\"\"\n",
    "        # Use log probabilities for numerical stability\n",
    "        log_posteriors = []\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Start with log prior\n",
    "            log_posterior = np.log(self.class_priors[idx] + 1e-10)\n",
    "\n",
    "            # Add log probabilities for each feature\n",
    "            for i in range(len(x)):\n",
    "                prob = self.get_bin_probability(x[i], c, i)\n",
    "                log_posterior += np.log(prob + 1e-10)\n",
    "\n",
    "            log_posteriors.append(log_posterior)\n",
    "\n",
    "        return self.classes[np.argmax(log_posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict multiple instances\n",
    "        \"\"\"\n",
    "        return np.array([self.predict_single(x) for x in X])"
   ],
   "id": "ae898492313359c9",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "K-FOLD Cross Val:",
   "id": "4435aa1d5cbc278b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T19:24:58.619902Z",
     "start_time": "2025-10-11T19:24:52.744686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model_kfold(model_class, X, y, n_folds=5):\n",
    "    \"\"\"\n",
    "    Evaluate model using k-fold cross validation\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "        # Split data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Normalize data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_norm = scaler.fit_transform(X_train)\n",
    "        X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "        # Train model\n",
    "        model = model_class()\n",
    "        model.fit(X_train_norm, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred_train = model.predict(X_train_norm)\n",
    "        y_pred_test = model.predict(X_test_norm)\n",
    "\n",
    "        train_acc = accuracy_score(y_train, y_pred_train)\n",
    "        test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        print(f\"Fold {fold}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "\n",
    "    avg_train = np.mean(train_accuracies)\n",
    "    avg_test = np.mean(test_accuracies)\n",
    "    std_train = np.std(train_accuracies)\n",
    "    std_test = np.std(test_accuracies)\n",
    "\n",
    "    return {\n",
    "        'train_accs': train_accuracies,\n",
    "        'test_accs': test_accuracies,\n",
    "        'avg_train': avg_train,\n",
    "        'avg_test': avg_test,\n",
    "        'std_train': std_train,\n",
    "        'std_test': std_test\n",
    "    }\n",
    "\n",
    "# Evaluate all three models\n",
    "print(\"=\"*50)\n",
    "print(\"Gaussian Naive Bayes Results:\")\n",
    "print(\"=\"*50)\n",
    "gnb_results = evaluate_model_kfold(GaussianNaiveBayes, X, y, n_folds=5)\n",
    "print(f\"\\nAverage Train Accuracy: {gnb_results['avg_train']:.4f} ± {gnb_results['std_train']:.4f}\")\n",
    "print(f\"Average Test Accuracy: {gnb_results['avg_test']:.4f} ± {gnb_results['std_test']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Bernoulli Naive Bayes Results:\")\n",
    "print(\"=\"*50)\n",
    "bnb_results = evaluate_model_kfold(BernoulliNaiveBayes, X, y, n_folds=5)\n",
    "print(f\"\\nAverage Train Accuracy: {bnb_results['avg_train']:.4f} ± {bnb_results['std_train']:.4f}\")\n",
    "print(f\"Average Test Accuracy: {bnb_results['avg_test']:.4f} ± {bnb_results['std_test']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Histogram Naive Bayes Results:\")\n",
    "print(\"=\"*50)\n",
    "hnb_results = evaluate_model_kfold(HistogramNaiveBayes, X, y, n_folds=5)\n",
    "print(f\"\\nAverage Train Accuracy: {hnb_results['avg_train']:.4f} ± {hnb_results['std_train']:.4f}\")\n",
    "print(f\"Average Test Accuracy: {hnb_results['avg_test']:.4f} ± {hnb_results['std_test']:.4f}\")"
   ],
   "id": "ee385507d47c0112",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Gaussian Naive Bayes Results:\n",
      "==================================================\n",
      "Fold 1: Train Acc = 0.8204, Test Acc = 0.8208\n",
      "Fold 2: Train Acc = 0.8047, Test Acc = 0.8033\n",
      "Fold 3: Train Acc = 0.8245, Test Acc = 0.7946\n",
      "Fold 4: Train Acc = 0.8250, Test Acc = 0.8228\n",
      "Fold 5: Train Acc = 0.8215, Test Acc = 0.8337\n",
      "\n",
      "Average Train Accuracy: 0.8192 ± 0.0075\n",
      "Average Test Accuracy: 0.8150 ± 0.0141\n",
      "\n",
      "==================================================\n",
      "Bernoulli Naive Bayes Results:\n",
      "==================================================\n",
      "Fold 1: Train Acc = 0.8938, Test Acc = 0.9001\n",
      "Fold 2: Train Acc = 0.8894, Test Acc = 0.8848\n",
      "Fold 3: Train Acc = 0.8970, Test Acc = 0.8880\n",
      "Fold 4: Train Acc = 0.8998, Test Acc = 0.8783\n",
      "Fold 5: Train Acc = 0.8946, Test Acc = 0.9065\n",
      "\n",
      "Average Train Accuracy: 0.8949 ± 0.0034\n",
      "Average Test Accuracy: 0.8915 ± 0.0103\n",
      "\n",
      "==================================================\n",
      "Histogram Naive Bayes Results:\n",
      "==================================================\n",
      "Fold 1: Train Acc = 0.7367, Test Acc = 0.7242\n",
      "Fold 2: Train Acc = 0.7571, Test Acc = 0.7457\n",
      "Fold 3: Train Acc = 0.7558, Test Acc = 0.7620\n",
      "Fold 4: Train Acc = 0.7590, Test Acc = 0.7717\n",
      "Fold 5: Train Acc = 0.7571, Test Acc = 0.7652\n",
      "\n",
      "Average Train Accuracy: 0.7532 ± 0.0083\n",
      "Average Test Accuracy: 0.7538 ± 0.0171\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T19:25:01.617107Z",
     "start_time": "2025-10-11T19:24:58.630511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the fixed Histogram NB\n",
    "print(\"=\"*50)\n",
    "print(\"Fixed Histogram Naive Bayes Results:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use the alternative implementation\n",
    "hnb_fixed_results = evaluate_model_kfold(HistogramNaiveBayes, X, y, n_folds=5)\n",
    "\n",
    "print(f\"\\nAverage Train Accuracy: {hnb_fixed_results['avg_train']:.4f} ± {hnb_fixed_results['std_train']:.4f}\")\n",
    "print(f\"Average Test Accuracy: {hnb_fixed_results['avg_test']:.4f} ± {hnb_fixed_results['std_test']:.4f}\")\n",
    "\n",
    "# Create updated summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY TABLE - All Models (5-Fold CV)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Train Acc':<15} {'Test Acc':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Gaussian NB':<20} {gnb_results['avg_train']:.4f} ± {gnb_results['std_train']:.4f}  \"\n",
    "      f\"{gnb_results['avg_test']:.4f} ± {gnb_results['std_test']:.4f}\")\n",
    "print(f\"{'Bernoulli NB':<20} {bnb_results['avg_train']:.4f} ± {bnb_results['std_train']:.4f}  \"\n",
    "      f\"{bnb_results['avg_test']:.4f} ± {bnb_results['std_test']:.4f}\")\n",
    "print(f\"{'Histogram NB (Fixed)':<20} {hnb_fixed_results['avg_train']:.4f} ± {hnb_fixed_results['std_train']:.4f}  \"\n",
    "      f\"{hnb_fixed_results['avg_test']:.4f} ± {hnb_fixed_results['std_test']:.4f}\")"
   ],
   "id": "4f804495e9ab6422",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Fixed Histogram Naive Bayes Results:\n",
      "==================================================\n",
      "Fold 1: Train Acc = 0.7367, Test Acc = 0.7242\n",
      "Fold 2: Train Acc = 0.7571, Test Acc = 0.7457\n",
      "Fold 3: Train Acc = 0.7558, Test Acc = 0.7620\n",
      "Fold 4: Train Acc = 0.7590, Test Acc = 0.7717\n",
      "Fold 5: Train Acc = 0.7571, Test Acc = 0.7652\n",
      "\n",
      "Average Train Accuracy: 0.7532 ± 0.0083\n",
      "Average Test Accuracy: 0.7538 ± 0.0171\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY TABLE - All Models (5-Fold CV)\n",
      "============================================================\n",
      "Model                Train Acc       Test Acc       \n",
      "------------------------------------------------------------\n",
      "Gaussian NB          0.8192 ± 0.0075  0.8150 ± 0.0141\n",
      "Bernoulli NB         0.8949 ± 0.0034  0.8915 ± 0.0103\n",
      "Histogram NB (Fixed) 0.7532 ± 0.0083  0.7538 ± 0.0171\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T19:25:16.213379Z",
     "start_time": "2025-10-11T19:25:01.636571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test with different alpha values and k-folds\n",
    "def test_hyperparameters(X, y):\n",
    "    \"\"\"\n",
    "    Test different hyperparameter combinations\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Test different alpha (smoothing) values\n",
    "    alphas = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "    k_folds = [3, 5, 10]\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"Hyperparameter Tuning for Histogram Naive Bayes\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for k in k_folds:\n",
    "        for alpha in alphas:\n",
    "            # Create model with specific alpha\n",
    "            class HNBWithAlpha(HistogramNaiveBayes):\n",
    "                def __init__(self):\n",
    "                    super().__init__(n_bins=4, alpha=alpha)\n",
    "\n",
    "            # Run k-fold CV\n",
    "            kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "            test_scores = []\n",
    "\n",
    "            for train_idx, test_idx in kf.split(X):\n",
    "                X_train, X_test = X[train_idx], X[test_idx]\n",
    "                y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "                # Normalize\n",
    "                scaler = StandardScaler()\n",
    "                X_train_norm = scaler.fit_transform(X_train)\n",
    "                X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "                # Train and predict\n",
    "                model = HNBWithAlpha()\n",
    "                model.fit(X_train_norm, y_train)\n",
    "                y_pred = model.predict(X_test_norm)\n",
    "\n",
    "                test_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "            avg_score = np.mean(test_scores)\n",
    "            std_score = np.std(test_scores)\n",
    "\n",
    "            print(f\"k={k:2d}, alpha={alpha:4.2f}: Test Acc = {avg_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = {'k_folds': k, 'alpha': alpha, 'std': std_score}\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Best Parameters: k={best_params['k_folds']}, alpha={best_params['alpha']:.2f}\")\n",
    "    print(f\"Best Test Accuracy: {best_score:.4f} ± {best_params['std']:.4f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "best_params = test_hyperparameters(X, y)\n",
    "\n",
    "# Run final evaluation with best parameters\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Final Evaluation with Best Parameters\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class OptimizedHNB(HistogramNaiveBayes):\n",
    "    def __init__(self):\n",
    "        super().__init__(n_bins=4, alpha=best_params['alpha'])\n",
    "\n",
    "final_results = evaluate_model_kfold(OptimizedHNB, X, y, n_folds=best_params['k_folds'])\n",
    "print(f\"Final Train Accuracy: {final_results['avg_train']:.4f} ± {final_results['std_train']:.4f}\")\n",
    "print(f\"Final Test Accuracy: {final_results['avg_test']:.4f} ± {final_results['std_test']:.4f}\")"
   ],
   "id": "f56b25869f38dba2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Hyperparameter Tuning for Histogram Naive Bayes\n",
      "======================================================================\n",
      "k= 3, alpha=0.01: Test Acc = 0.7609 ± 0.0098\n",
      "k= 3, alpha=0.10: Test Acc = 0.7596 ± 0.0116\n",
      "k= 3, alpha=0.50: Test Acc = 0.7577 ± 0.0128\n",
      "k= 3, alpha=1.00: Test Acc = 0.7553 ± 0.0139\n",
      "k= 3, alpha=2.00: Test Acc = 0.7501 ± 0.0134\n",
      "k= 3, alpha=5.00: Test Acc = 0.7390 ± 0.0154\n",
      "k= 5, alpha=0.01: Test Acc = 0.7618 ± 0.0137\n",
      "k= 5, alpha=0.10: Test Acc = 0.7605 ± 0.0138\n",
      "k= 5, alpha=0.50: Test Acc = 0.7561 ± 0.0166\n",
      "k= 5, alpha=1.00: Test Acc = 0.7538 ± 0.0171\n",
      "k= 5, alpha=2.00: Test Acc = 0.7485 ± 0.0177\n",
      "k= 5, alpha=5.00: Test Acc = 0.7416 ± 0.0205\n",
      "k=10, alpha=0.01: Test Acc = 0.7533 ± 0.0214\n",
      "k=10, alpha=0.10: Test Acc = 0.7496 ± 0.0243\n",
      "k=10, alpha=0.50: Test Acc = 0.7455 ± 0.0239\n",
      "k=10, alpha=1.00: Test Acc = 0.7424 ± 0.0259\n",
      "k=10, alpha=2.00: Test Acc = 0.7392 ± 0.0267\n",
      "k=10, alpha=5.00: Test Acc = 0.7314 ± 0.0292\n",
      "\n",
      "======================================================================\n",
      "Best Parameters: k=5, alpha=0.01\n",
      "Best Test Accuracy: 0.7618 ± 0.0137\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Final Evaluation with Best Parameters\n",
      "======================================================================\n",
      "Fold 1: Train Acc = 0.7473, Test Acc = 0.7394\n",
      "Fold 2: Train Acc = 0.7656, Test Acc = 0.7522\n",
      "Fold 3: Train Acc = 0.7675, Test Acc = 0.7707\n",
      "Fold 4: Train Acc = 0.7645, Test Acc = 0.7739\n",
      "Fold 5: Train Acc = 0.7664, Test Acc = 0.7728\n",
      "Final Train Accuracy: 0.7622 ± 0.0075\n",
      "Final Test Accuracy: 0.7618 ± 0.0137\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T19:26:29.816194Z",
     "start_time": "2025-10-11T19:26:21.273579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HistogramNBSpambaseOptimized:\n",
    "    def __init__(self, n_bins=4, alpha=0.01, global_bins=False):\n",
    "        \"\"\"\n",
    "        Optimized for Spambase dataset characteristics\n",
    "        global_bins: If True, use global min/max for binning instead of class-specific\n",
    "        \"\"\"\n",
    "        self.n_bins = n_bins\n",
    "        self.alpha = alpha\n",
    "        self.global_bins = global_bins\n",
    "        self.class_priors = None\n",
    "        self.bin_edges = {}\n",
    "        self.bin_probs = {}\n",
    "        self.classes = None\n",
    "        self.global_edges = {}  # Store global edges if needed\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train with special handling for Spambase features\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.class_priors = {}\n",
    "\n",
    "        # If using global bins, compute global edges first\n",
    "        if self.global_bins:\n",
    "            for feature_idx in range(n_features):\n",
    "                feature_values = X[:, feature_idx]\n",
    "\n",
    "                # Handle features with many zeros (common in Spambase)\n",
    "                non_zero_values = feature_values[feature_values > 0]\n",
    "\n",
    "                if len(non_zero_values) > 10:  # If we have enough non-zero values\n",
    "                    # Use percentiles of non-zero values\n",
    "                    min_val = 0  # Include zero as minimum\n",
    "                    q1 = np.percentile(non_zero_values, 25)\n",
    "                    mean_val = np.mean(non_zero_values)\n",
    "                    q3 = np.percentile(non_zero_values, 75)\n",
    "                    max_val = np.max(non_zero_values)\n",
    "                else:\n",
    "                    # Use regular percentiles\n",
    "                    min_val = np.min(feature_values)\n",
    "                    q1 = np.percentile(feature_values, 25)\n",
    "                    mean_val = np.mean(feature_values)\n",
    "                    q3 = np.percentile(feature_values, 75)\n",
    "                    max_val = np.max(feature_values)\n",
    "\n",
    "                edges = np.sort([min_val, q1, mean_val, q3, max_val])\n",
    "                edges = np.unique(edges)\n",
    "\n",
    "                if len(edges) < self.n_bins + 1:\n",
    "                    edges = np.linspace(min_val, max_val + 1e-10, self.n_bins + 1)\n",
    "\n",
    "                self.global_edges[feature_idx] = edges\n",
    "\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.class_priors[c] = len(X_c) / n_samples\n",
    "\n",
    "            self.bin_edges[c] = {}\n",
    "            self.bin_probs[c] = {}\n",
    "\n",
    "            for feature_idx in range(n_features):\n",
    "                if self.global_bins:\n",
    "                    # Use global edges\n",
    "                    edges = self.global_edges[feature_idx].copy()\n",
    "                else:\n",
    "                    # Class-specific edges\n",
    "                    feature_values = X_c[:, feature_idx]\n",
    "\n",
    "                    # Special handling for sparse features\n",
    "                    zero_ratio = np.mean(feature_values == 0)\n",
    "\n",
    "                    if zero_ratio > 0.5:  # If more than 50% zeros\n",
    "                        # Create bins that separate zeros from non-zeros\n",
    "                        non_zero_values = feature_values[feature_values > 0]\n",
    "                        if len(non_zero_values) > 3:\n",
    "                            edges = [0,\n",
    "                                   np.min(non_zero_values),\n",
    "                                   np.median(non_zero_values),\n",
    "                                   np.max(non_zero_values) + 1e-10]\n",
    "                        else:\n",
    "                            edges = np.linspace(np.min(feature_values),\n",
    "                                              np.max(feature_values) + 1e-10,\n",
    "                                              self.n_bins + 1)\n",
    "                    else:\n",
    "                        # Standard approach\n",
    "                        min_val = np.min(feature_values)\n",
    "                        q1 = np.percentile(feature_values, 25)\n",
    "                        mean_val = np.mean(feature_values)\n",
    "                        q3 = np.percentile(feature_values, 75)\n",
    "                        max_val = np.max(feature_values)\n",
    "\n",
    "                        edges = np.sort([min_val, q1, mean_val, q3, max_val])\n",
    "                        edges = np.unique(edges)\n",
    "\n",
    "                        if len(edges) < self.n_bins + 1:\n",
    "                            edges = np.linspace(min_val, max_val + 1e-10, self.n_bins + 1)\n",
    "\n",
    "                edges[-1] += 1e-10  # Ensure max is included\n",
    "\n",
    "                # Calculate histogram\n",
    "                hist, _ = np.histogram(X_c[:, feature_idx], bins=edges)\n",
    "\n",
    "                # Adaptive smoothing based on sample size\n",
    "                adaptive_alpha = self.alpha * (1000 / len(X_c))  # Scale smoothing by sample size\n",
    "                hist_smooth = hist + adaptive_alpha\n",
    "\n",
    "                # Normalize\n",
    "                hist_prob = hist_smooth / np.sum(hist_smooth)\n",
    "\n",
    "                self.bin_edges[c][feature_idx] = edges\n",
    "                self.bin_probs[c][feature_idx] = hist_prob\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict with log probabilities\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "            log_posteriors = {}\n",
    "\n",
    "            for c in self.classes:\n",
    "                log_post = np.log(self.class_priors[c] + 1e-10)\n",
    "\n",
    "                for j in range(len(x)):\n",
    "                    edges = self.bin_edges[c][j]\n",
    "                    probs = self.bin_probs[c][j]\n",
    "\n",
    "                    # Find bin\n",
    "                    bin_idx = np.searchsorted(edges[:-1], x[j], side='right') - 1\n",
    "                    bin_idx = np.clip(bin_idx, 0, len(probs) - 1)\n",
    "\n",
    "                    log_post += np.log(probs[bin_idx] + 1e-10)\n",
    "\n",
    "                log_posteriors[c] = log_post\n",
    "\n",
    "            predictions.append(max(log_posteriors, key=log_posteriors.get))\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Test different configurations\n",
    "print(\"=\"*70)\n",
    "print(\"Testing Different Histogram Configurations\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "configurations = [\n",
    "    {\"n_bins\": 4, \"alpha\": 0.01, \"global_bins\": False, \"name\": \"Original (4 bins, class-specific)\"},\n",
    "    {\"n_bins\": 4, \"alpha\": 0.01, \"global_bins\": True, \"name\": \"Global bins\"},\n",
    "    {\"n_bins\": 8, \"alpha\": 0.01, \"global_bins\": False, \"name\": \"8 bins\"},\n",
    "    {\"n_bins\": 10, \"alpha\": 0.01, \"global_bins\": False, \"name\": \"10 bins\"},\n",
    "    {\"n_bins\": 16, \"alpha\": 0.01, \"global_bins\": False, \"name\": \"16 bins\"},\n",
    "]\n",
    "\n",
    "best_config = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for config in configurations:\n",
    "    class TestHNB(HistogramNBSpambaseOptimized):\n",
    "        def __init__(self):\n",
    "            super().__init__(n_bins=config[\"n_bins\"],\n",
    "                           alpha=config[\"alpha\"],\n",
    "                           global_bins=config[\"global_bins\"])\n",
    "\n",
    "    # Quick 3-fold test\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_norm = scaler.fit_transform(X_train)\n",
    "        X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "        model = TestHNB()\n",
    "        model.fit(X_train_norm, y_train)\n",
    "        y_pred = model.predict(X_test_norm)\n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    print(f\"{config['name']:<35} Test Acc: {avg_score:.4f}\")\n",
    "\n",
    "    if avg_score > best_accuracy:\n",
    "        best_accuracy = avg_score\n",
    "        best_config = config\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Best Configuration: {best_config['name']}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "print(\"=\"*70)"
   ],
   "id": "7fbc285e36963cc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Testing Different Histogram Configurations\n",
      "======================================================================\n",
      "Original (4 bins, class-specific)   Test Acc: 0.7609\n",
      "Global bins                         Test Acc: 0.6449\n",
      "8 bins                              Test Acc: 0.8420\n",
      "10 bins                             Test Acc: 0.8624\n",
      "16 bins                             Test Acc: 0.8789\n",
      "\n",
      "======================================================================\n",
      "Best Configuration: 16 bins\n",
      "Best Accuracy: 0.8789\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "71cacea211a75f5f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
