{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb93ec22b123498",
   "metadata": {},
   "source": [
    "## HW2"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# housing dataset:\n",
    "train_house = np.loadtxt(\"housing_data/train.txt\")\n",
    "test_house = np.loadtxt(\"housing_data/test.txt\")\n",
    "\n",
    "# spambase dataset:\n",
    "spambase_data = np.loadtxt(\"spambase/spambase.data\", delimiter=\",\")"
   ],
   "id": "320741bb73fe9e40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Q1 A - Housing dataset with Linear Reg (normal eqns)",
   "id": "1429e5c759dabfd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. separate features and labels\n",
    "X_train = train_house[:, :-1]\n",
    "y_train = train_house[:, -1]\n",
    "X_test = test_house[:, :-1]\n",
    "y_test = test_house[:, -1]\n",
    "\n",
    "# 2. Normalization step\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# 3. bias step\n",
    "ones_col_Xtrain = np.ones((X_train_normalized.shape[0], 1))\n",
    "ones_col_Xtest = np.ones((X_test_normalized.shape[0], 1))\n",
    "\n",
    "Xtrain_house_bias = np.hstack([ones_col_Xtrain, X_train_normalized])\n",
    "Xtest_house_bias = np.hstack([ones_col_Xtest, X_test_normalized])\n",
    "\n",
    "# 4. Normal Eqn step\n",
    "Xtrain_T = Xtrain_house_bias.T\n",
    "Xtrain_dot = np.dot(Xtrain_T, Xtrain_house_bias)\n",
    "Xtrain_inv = np.linalg.inv(Xtrain_dot)\n",
    "Xtrain_y_dot = np.dot(Xtrain_T, y_train)\n",
    "theta_1 = np.dot(Xtrain_inv, Xtrain_y_dot)\n",
    "\n",
    "# 5. Prediction step\n",
    "train_house_pred = np.dot(Xtrain_house_bias, theta_1)\n",
    "test_house_pred = np.dot(Xtest_house_bias, theta_1)\n",
    "\n",
    "# 6. MSE for linear reg with normal equations\n",
    "train_mse_1 = np.mean((train_house_pred-y_train)**2)\n",
    "test_mse_1 = np.mean((test_house_pred-y_test)**2)\n",
    "\n",
    "print(f\"TRAIN-MSE-1 Housing data with Linear Reg: {train_mse_1}\")\n",
    "print(f\"TEST-MSE-1 Housing data with Linear Reg: {test_mse_1}\")"
   ],
   "id": "fe64aeeb7df34b85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Q1 B - Housing dataset with Linear Ridge Reg ->\n",
    "$ \\theta = (X^TX + \\lambda I) X^T Y $"
   ],
   "id": "2f8eba67ca8c2eb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lambdas = np.arange(0, 2.1, 0.1)\n",
    "test_mses = []\n",
    "train_mses = []\n",
    "# using normalized and bias data from Q1 A part\n",
    "\n",
    "for lambda_ridge in lambdas:\n",
    "    I = np.eye(Xtrain_house_bias.shape[1])\n",
    "    I[0,0] = 0\n",
    "\n",
    "    # ridge regression by normal equation\n",
    "    Xtrain_T = Xtrain_house_bias.T\n",
    "    Xtrain_dot_ridge = np.dot(Xtrain_T, Xtrain_house_bias) + lambda_ridge*I\n",
    "    Xtrain_inv_ridge = np.linalg.inv(Xtrain_dot_ridge)\n",
    "    Xtrain_y_dot = np.dot(Xtrain_T, y_train)\n",
    "    theta_2 = np.dot(Xtrain_inv_ridge, Xtrain_y_dot)\n",
    "\n",
    "    # predictions step\n",
    "    train_house_pred_ridge = np.dot(Xtrain_house_bias, theta_2)\n",
    "    test_house_pred_ridge = np.dot(Xtest_house_bias, theta_2)\n",
    "\n",
    "    # MSE for housing data with linear ridge regression\n",
    "    train_mse_2 = np.mean((train_house_pred_ridge-y_train)**2)\n",
    "    test_mse_2 = np.mean((test_house_pred_ridge-y_test)**2)\n",
    "\n",
    "    train_mses.append(train_mse_2)\n",
    "    test_mses.append(test_mse_2)\n",
    "\n",
    "    print(f\"Lambda: {lambda_ridge:.1f} | Train MSE-2: {train_mse_2:.4f} | Test MSE-2: {test_mse_2:.4f}\")\n",
    "\n",
    "# best lambda based on test MSE\n",
    "best_idx = np.argmin(test_mses)\n",
    "best_lambda = lambdas[best_idx]\n",
    "best_train_mse = train_mses[best_idx]\n",
    "best_test_mse = test_mses[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"BEST Lambda: {best_lambda:.1f}\")\n",
    "print(f\"Best Train MSE: {best_train_mse:.6f}\")\n",
    "print(f\"Best Test MSE: {best_test_mse:.6f}\")"
   ],
   "id": "ddd191708462bfc8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Q1 C - Housing dataset with Linear Reg (Gradient Descent)",
   "id": "281b160e363a3e6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def gradient_descent_lin_reg(X, y, lr=0.1, epochs=1000):\n",
    "    n,d = X.shape\n",
    "    w = np.zeros(d)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        predictions = X @ w\n",
    "        error = predictions - y\n",
    "\n",
    "        gradient = (1/n) * X.T @ error\n",
    "\n",
    "        w = w-lr*gradient\n",
    "\n",
    "        loss = np.mean(error**2)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        if epoch%100==0:\n",
    "            print(f\"Epoch {epoch}, MSE: {loss:.4f}\")\n",
    "\n",
    "    return w, loss_history"
   ],
   "id": "e8da4237e498499a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Train Linear Regression with Gradient Descent:\")\n",
    "w_gd, losses = gradient_descent_lin_reg(Xtrain_house_bias, y_train, lr=0.1, epochs=1000)\n",
    "\n",
    "# predictions step:\n",
    "train_pred_gd = Xtrain_house_bias @ w_gd\n",
    "test_pred_gd = Xtest_house_bias @ w_gd\n",
    "\n",
    "# MSE Calculation:\n",
    "train_mse_3 = np.mean((train_pred_gd-y_train) ** 2)\n",
    "test_mse_3 = np.mean((test_pred_gd-y_test) ** 2)\n",
    "\n",
    "print(f\"\\n Final Train MSE: {train_mse_3:.6f}\")\n",
    "print(f\"\\n Final Test MSE: {test_mse_3:.6f}\")\n",
    "\n",
    "# Convergence Plot:\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Gradient Descent Convergence')\n",
    "plt.show()"
   ],
   "id": "3ab0c9e4027f0681"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(losses[10:])\n",
    "plt.xlabel('Epoch (starting from 10)')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('GD Convergence (after initial drop)')\n",
    "plt.ylim(22.08, 22.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "1a3a188eb6846261"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Q1 D - Housing dataset with Logistic Reg (Gradient Descent) this won't work as for housing problem we need to predict continuous values ( house prices like 250k ,321k etc). But logistic regression gives us probabilities between 0 and 1 via the sigmoid function. It helps us predict discrete classes (spam/ not spam, pass/fail) not continuous values and is inappropriate for unbounded continuous values like house prices.",
   "id": "9ae82ea0925c8f50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Q1 E - Spambase dataset with Linear Regression (Normal Equations)",
   "id": "c1f8113038d19379"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = spambase_data[:, :-1]\n",
    "y = spambase_data[:, -1]\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalization step:\n",
    "scalar_spam = StandardScaler()\n",
    "scalar_spam.fit(X_train)\n",
    "X_train_normalized = scalar_spam.transform(X_train)\n",
    "X_test_normalized = scalar_spam.transform(X_test)\n",
    "\n",
    "# add bias term:\n",
    "ones_col_Xtrain = np.ones((X_train_normalized.shape[0], 1))\n",
    "ones_col_Xtest = np.ones((X_test_normalized.shape[0], 1))\n",
    "spam_Xtrain_bias = np.hstack([ones_col_Xtrain, X_train_normalized])\n",
    "spam_Xtest_bias = np.hstack([ones_col_Xtest, X_test_normalized])\n",
    "\n",
    "# Normal Equation step:\n",
    "spamXtrain_T = spam_Xtrain_bias.T\n",
    "spamXtrain_dot = spamXtrain_T @ spam_Xtrain_bias\n",
    "spamXtrain_inverse = np.linalg.inv(spamXtrain_dot)\n",
    "spamX_y_dot = spamXtrain_T @ y_train\n",
    "w_spam_linear = spamXtrain_inverse @ spamX_y_dot\n",
    "\n",
    "# predictions:\n",
    "train_continuous_pred = spam_Xtrain_bias @ w_spam_linear\n",
    "test_continuous_pred = spam_Xtest_bias @ w_spam_linear"
   ],
   "id": "12f597675ea4a374"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Binary classification with threshold:\n",
    "threshold = 0.42\n",
    "train_binary_pred = (train_continuous_pred>threshold).astype(int)\n",
    "test_binary_pred = (test_continuous_pred>threshold).astype(int)\n",
    "\n",
    "# Accuracy:\n",
    "train_acc_linear = np.mean(train_binary_pred == y_train)\n",
    "test_acc_linear = np.mean(test_binary_pred == y_test)\n",
    "\n",
    "print(f\"\\nLinear Regression on Spambase:\")\n",
    "print(f\"Train Accuracy: {train_acc_linear:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_linear:.4f}\")"
   ],
   "id": "c7bc5a7c94c00d4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Confusion Matrix\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return np.array([[TN, FP], [FN, TP]])\n",
    "\n",
    "cm_linear = confusion_matrix(y_test, test_binary_pred)\n",
    "print(f\"\\nConfusion Matrix (Linear Regression):\")\n",
    "print(f\"[[TN={cm_linear[0,0]} FP={cm_linear[0,1]}]\")\n",
    "print(f\" [FN={cm_linear[1,0]} TP={cm_linear[1,1]}]]\")"
   ],
   "id": "6bb2798274250448"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Q1 F - Spambase dataset with Linear Ridge Regression",
   "id": "71472cd57d245a96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Q1 F - Spambase dataset with Linear Ridge Regression\n",
    "\n",
    "# Testing different lambda values with fixed threshold value\n",
    "threshold = 0.42\n",
    "lambdas = np.arange(0, 10.1, 0.1)\n",
    "train_accs_ridge = []\n",
    "test_accs_ridge = []\n",
    "\n",
    "print(\"Testing Ridge Regression with different lambda values:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for lambda_ridge in lambdas:\n",
    "    I = np.eye(spam_Xtrain_bias.shape[1])\n",
    "    I[0, 0] = 0\n",
    "\n",
    "    spamXtrain_T = spam_Xtrain_bias.T\n",
    "    spamXtrain_dot_ridge = spamXtrain_T @ spam_Xtrain_bias + lambda_ridge * I\n",
    "    spamXtrain_inv_ridge = np.linalg.inv(spamXtrain_dot_ridge)\n",
    "    spamX_y_dot = spamXtrain_T @ y_train\n",
    "    w_spam_ridge = spamXtrain_inv_ridge @ spamX_y_dot\n",
    "\n",
    "    train_conti_ridge = spam_Xtrain_bias @ w_spam_ridge\n",
    "    test_conti_ridge = spam_Xtest_bias @ w_spam_ridge\n",
    "\n",
    "    train_binary_ridge = (train_conti_ridge > threshold).astype(int)\n",
    "    test_binary_ridge = (test_conti_ridge > threshold).astype(int)\n",
    "\n",
    "    train_acc = np.mean(train_binary_ridge == y_train)\n",
    "    test_acc = np.mean(test_binary_ridge == y_test)\n",
    "\n",
    "    train_accs_ridge.append(train_acc)\n",
    "    test_accs_ridge.append(test_acc)\n",
    "\n",
    "    print(f\"λ={lambda_ridge:.1f}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "\n",
    "# best lambda search:\n",
    "best_idx = np.argmax(test_accs_ridge)\n",
    "best_lambda = lambdas[best_idx]\n",
    "best_train_acc = train_accs_ridge[best_idx]\n",
    "best_test_acc = test_accs_ridge[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"BEST Lambda: {best_lambda:.1f}\")\n",
    "print(f\"Best Train Accuracy: {best_train_acc:.4f}\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f}\")\n",
    "\n",
    "# rerunning for-loop with the best lambda value found:\n",
    "I = np.eye(spam_Xtrain_bias.shape[1])\n",
    "I[0, 0] = 0\n",
    "spamXtrain_dot_ridge = np.dot(spamXtrain_T, spam_Xtrain_bias) + best_lambda * I\n",
    "spamXtrain_inv_ridge = np.linalg.inv(spamXtrain_dot_ridge)\n",
    "w_spam_ridge_best = np.dot(spamXtrain_inv_ridge, spamX_y_dot)\n",
    "\n",
    "test_pred_ridge = np.dot(spam_Xtest_bias, w_spam_ridge_best)\n",
    "test_binary_ridge = (test_pred_ridge > threshold).astype(int)"
   ],
   "id": "733b9e9be0b6a6f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Confusion Matrix 2\n",
    "cm_ridge = confusion_matrix(y_test, test_binary_ridge)\n",
    "print(f\"\\nConfusion Matrix (Ridge Regression, λ={best_lambda}):\")\n",
    "print(f\"[[TN={cm_ridge[0,0]} FP={cm_ridge[0,1]}]\")\n",
    "print(f\" [FN={cm_ridge[1,0]} TP={cm_ridge[1,1]}]]\")"
   ],
   "id": "d35b02d8b7855470"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Q1 G - Spambase dataset with Linear Regression using Gradient Descent",
   "id": "1be882c6e0088054"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def gradient_descent_linear_spam(X, y, lr=0.011, epochs=1000):\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = X @ w\n",
    "\n",
    "        # calculate gradient\n",
    "        error = y_pred - y\n",
    "        gradient = (1/n) * X.T @ error\n",
    "\n",
    "        # new weights are:\n",
    "        w = w - lr * gradient\n",
    "\n",
    "        # mse loss:\n",
    "        loss = np.mean(error**2)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, MSE: {loss:.4f}\")\n",
    "\n",
    "    return w, loss_history"
   ],
   "id": "585f7c6815510788"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# training part:\n",
    "w_spam_gd, losses_spam = gradient_descent_linear_spam(spam_Xtrain_bias, y_train, lr=0.011, epochs=1000)\n",
    "\n",
    "# predictions:\n",
    "train_pred_gd_spam = spam_Xtrain_bias @ w_spam_gd\n",
    "test_pred_gd_spam = spam_Xtest_bias @ w_spam_gd\n",
    "\n",
    "train_binary_gd = (train_pred_gd_spam > threshold).astype(int)\n",
    "test_binary_gd = (test_pred_gd_spam > threshold).astype(int)\n",
    "\n",
    "# accuracy metric:\n",
    "train_acc_gd = np.mean(train_binary_gd == y_train)\n",
    "test_acc_gd = np.mean(test_binary_gd == y_test)\n",
    "\n",
    "print(f\"\\nLinear Regression (Gradient Descent) on Spambase:\")\n",
    "print(f\"Train Accuracy: {train_acc_gd:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_gd:.4f}\")"
   ],
   "id": "2d91594b14f9af25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cm_gd = confusion_matrix(y_test, test_binary_gd)\n",
    "print(f\"\\nConfusion Matrix (Linear Reg - Gradient Descent):\")\n",
    "print(f\"[[TN={cm_gd[0,0]} FP={cm_gd[0,1]}]\")\n",
    "print(f\" [FN={cm_gd[1,0]} TP={cm_gd[1,1]}]]\")"
   ],
   "id": "7271f68b8f89d4c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses_spam)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('GD Convergence - Spambase')\n",
    "plt.grid(True, alpha=0.3)"
   ],
   "id": "927360924fccfba2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(losses_spam[50:])\n",
    "plt.xlabel('Epoch (from 50)')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('GD Convergence - Zoomed')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8d0c6ce7a17a7888"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"\\nComparison:\")\n",
    "print(f\"Normal Equations: Test Acc = {test_acc_linear:.4f}\")\n",
    "print(f\"Gradient Descent: Test Acc = {test_acc_gd:.4f}\")"
   ],
   "id": "a6605eb565ad9672"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Q1G - Linear Regression with Gradient Descent (with learning rate optimization)\n",
    "\n",
    "learning_rates = np.arange(0, 0.01, 0.0001)\n",
    "train_accs_gd = []\n",
    "test_accs_gd = []\n",
    "\n",
    "print(\"Testing different learning rates for Linear Regression (GD):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Train with gradient descent\n",
    "    n, d = spam_Xtrain_bias.shape\n",
    "    w = np.zeros(d)\n",
    "\n",
    "    # Run gradient descent\n",
    "    for epoch in range(1000):\n",
    "        y_pred = spam_Xtrain_bias @ w\n",
    "        gradient = (1/n) * spam_Xtrain_bias.T @ (y_pred - y_train)\n",
    "        w = w - lr * gradient\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = spam_Xtrain_bias @ w\n",
    "    test_pred = spam_Xtest_bias @ w\n",
    "\n",
    "    # Convert to binary\n",
    "    train_binary = (train_pred > threshold).astype(int)\n",
    "    test_binary = (test_pred > threshold).astype(int)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    train_acc = np.mean(train_binary == y_train)\n",
    "    test_acc = np.mean(test_binary == y_test)\n",
    "\n",
    "    train_accs_gd.append(train_acc)\n",
    "    test_accs_gd.append(test_acc)\n",
    "\n",
    "    print(f\"LR={lr:.4f}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "\n",
    "# Find best learning rate\n",
    "best_idx = np.argmax(test_accs_gd)\n",
    "best_lr = learning_rates[best_idx]\n",
    "best_train_acc = train_accs_gd[best_idx]\n",
    "best_test_acc = test_accs_gd[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"BEST Learning Rate: {best_lr:.4f}\")\n",
    "print(f\"Best Train Accuracy: {best_train_acc:.4f}\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(learning_rates, train_accs_gd, 'b-', label='Train Accuracy')\n",
    "plt.plot(learning_rates, test_accs_gd, 'r-', label='Test Accuracy')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Rate vs Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "64112d9436110911"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Q1 H - Spambase dataset with Logistic Regression using Gradient descent",
   "id": "a5d03424d672c11b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def gradient_descent_logistic(X, y, lr=0.01, epochs=1000):\n",
    "    n, d = X.shape\n",
    "    w =np.zeros(d)\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        z = X @ w\n",
    "        y_pred = sigmoid(z)\n",
    "\n",
    "        # gradient\n",
    "        gradient = (1/n) * X.T @ (y_pred - y)\n",
    "\n",
    "        w = w - lr * gradient\n",
    "\n",
    "        # log likelihood\n",
    "        epsilon = 1e-7\n",
    "        loss = -np.mean(y*np.log(y_pred+epsilon)+(1-y)*np.log(1-y_pred+epsilon))\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        if epoch%100==0:\n",
    "            print(f\"Epoch {epoch}, Cross-Entropy Loss: {loss:.4f}\")\n",
    "    return w, loss_history"
   ],
   "id": "2d960edc99f28bea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "w_logistic, losses_logistic = gradient_descent_logistic(spam_Xtrain_bias, y_train, lr=0.5, epochs=1000)\n",
    "\n",
    "train_pred_prob = sigmoid(spam_Xtrain_bias @ w_logistic)\n",
    "test_pred_prob = sigmoid(spam_Xtest_bias @ w_logistic)\n",
    "\n",
    "train_binary_logistic = (train_pred_prob > 0.5).astype(int)\n",
    "test_binary_logistic = (test_pred_prob > 0.5).astype(int)\n",
    "\n",
    "train_acc_logistic = np.mean(train_binary_logistic == y_train)\n",
    "test_acc_logistic = np.mean(test_binary_logistic == y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc_logistic:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_logistic:.4f}\")"
   ],
   "id": "1f691d74dfc8c28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cm_logistic = confusion_matrix(y_test, test_binary_logistic)\n",
    "print(f\"\\nConfusion Matrix (Logistic Regression):\")\n",
    "print(f\"[[TN={cm_logistic[0,0]} FP={cm_logistic[0,1]}]\")\n",
    "print(f\" [FN={cm_logistic[1,0]} TP={cm_logistic[1,1]}]]\")"
   ],
   "id": "8d07260f2e1c2aca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses_logistic)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Logistic Regression Convergence')\n",
    "plt.grid(True, alpha=0.3)"
   ],
   "id": "da970d60a1d5410c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL COMPARISON ON SPAMBASE:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Linear Regression (Normal Eq):  Test Acc = {test_acc_linear:.4f}\")\n",
    "print(f\"Ridge Regression (λ=5.7):        Test Acc = {0.9121:.4f}\")  # Your best\n",
    "print(f\"Linear Regression (GD):          Test Acc = {test_acc_gd:.4f}\")\n",
    "print(f\"Logistic Regression (GD):        Test Acc = {test_acc_logistic:.4f}\")"
   ],
   "id": "3fb33afae752039"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ROC Curves and AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# For Linear Regression\n",
    "fpr_linear, tpr_linear, _ = roc_curve(y_test, test_continuous_pred)\n",
    "auc_linear = auc(fpr_linear, tpr_linear)\n",
    "\n",
    "# For Logistic Regression\n",
    "fpr_logistic, tpr_logistic, _ = roc_curve(y_test, test_pred_prob)\n",
    "auc_logistic = auc(fpr_logistic, tpr_logistic)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_linear, tpr_linear, label=f'Linear Regression (AUC = {auc_linear:.3f})')\n",
    "plt.plot(fpr_logistic, tpr_logistic, label=f'Logistic Regression (AUC = {auc_logistic:.3f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves: Linear vs Logistic Regression')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC - Linear Regression: {auc_linear:.4f}\")\n",
    "print(f\"AUC - Logistic Regression: {auc_logistic:.4f}\")"
   ],
   "id": "f33eae046a3d0232"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bd17ac068b0550a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
